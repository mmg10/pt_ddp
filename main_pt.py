import os
from timeit import default_timer as timer
from datetime import datetime
import yaml
import pandas as pd
from sklearn.model_selection import train_test_split

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


from transformers import BertTokenizer, BertConfig

from model import ModelClass
from trainer import train
from dataset import datasets

# Set random seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

with open("config.yaml", "r", encoding='utf_8') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_EPOCHS = config["NUM_EPOCHS"]
LR = float(config["LR"])
TRAIN_BATCH = config["TRAIN_BATCH"]
VAL_BATCH = config["VAL_BATCH"]
NUM_WORKERS = config["NUM_WORKERS"]

TRAIN_DATA = pd.read_csv( config["TRAIN_DATA"])
VAL_DATA = pd.read_csv( config["VAL_DATA"])
TRAIN_DATA = TRAIN_DATA[:1024]
VAL_DATA = VAL_DATA[:512]

# TEST_DATA = pd.read_csv( config["TEST_DATA"])
MAX_LEN = config["MAX_LEN"]
MODEL = config["MODEL"]
NUM_LABELS = config["NUM_LABELS"]

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Dataset
from datasets import load_dataset

emotions = load_dataset("emotion")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)


def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

emotions.set_format(type="pandas")
train_df = emotions["train"][:]
val_df = emotions["validation"][:]
train_df.rename(columns = {'label':'targets'}, inplace = True)
val_df.rename(columns = {'label':'targets'}, inplace = True)




if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    if n_gpus < 2:
        print(f"Requires at least 2 GPUs to run, but got {n_gpus}.")
    else:
        #######################
        # generated by torchrun
        #######################
        local_rank = int(os.environ["LOCAL_RANK"])
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])

        # initialize the process group
        # dist.init_process_group("nccl", rank=rank, world_size=world_size) # extra args taken care by torchrun 
        dist.init_process_group("nccl")

        run_id = datetime.now().strftime("%Y-%m-%d-%I:%M:%S_%p")

        train_dl, val_dl = datasets(train_df, TRAIN_BATCH, val_df, VAL_BATCH, tokenizer, MAX_LEN, rank, world_size)

        # Set up model
        model_class = ModelClass(model_name=MODEL, num_labels=NUM_LABELS, len_train_dl=len(train_dl), lr=LR, epochs=NUM_EPOCHS)

        model = model_class.model
        optimizer, scheduler = model_class.opt_sch()

        if rank == 0:
            print(f"--> running with id {run_id}")
            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"--> {MODEL} has {total_params/1e6} Million params\n")

        start_time = timer()

        model_0_results = train(
            model=model,
            train_dataloader=train_dl,
            val_dataloader=val_dl,
            optimizer=optimizer,
            scheduler=scheduler,
            device=DEVICE,
            epochs=NUM_EPOCHS,
        )

        # End the timer and print out how long it took
        end_time = timer()
        print(f"Total training time: {end_time-start_time:.3f} seconds")