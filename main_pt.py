import os
from socket import gethostname
from timeit import default_timer as timer
from datetime import datetime
import yaml
import pandas as pd
from sklearn.model_selection import train_test_split

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


from transformers import BertTokenizer, BertConfig

from model import ModelClass
from trainer import TrainerClass
from dataset import datasets

# Set random seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

with open("config.yaml", "r", encoding='utf_8') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


NUM_EPOCHS = config["NUM_EPOCHS"]
LR = float(config["LR"])
TRAIN_BATCH = config["TRAIN_BATCH"]
VAL_BATCH = config["VAL_BATCH"]
# NUM_WORKERS = config["NUM_WORKERS"]
PROFILE = config["PROFILE"]

NUM_WORKERS = os.cpu_count() / int(os.environ['LOCAL_WORLD_SIZE'])
os.environ["OMP_NUM_THREADS"] = str(NUM_WORKERS)



# TRAIN_DATA = pd.read_csv( config["TRAIN_DATA"])
# VAL_DATA = pd.read_csv( config["VAL_DATA"])
# TRAIN_DATA = TRAIN_DATA[:1024]
# VAL_DATA = VAL_DATA[:512]

# TEST_DATA = pd.read_csv( config["TEST_DATA"])
MAX_LEN = config["MAX_LEN"]
MODEL = config["MODEL"]
NUM_LABELS = config["NUM_LABELS"]

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Dataset
from datasets import load_dataset

emotions = load_dataset("emotion")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)


# def tokenize(batch):
#     return tokenizer(batch["text"], padding=True, truncation=True)

# emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

emotions.set_format(type="pandas")
train_df = emotions["train"][:]
val_df = emotions["validation"][:]
train_df.rename(columns = {'label':'targets'}, inplace = True)
val_df.rename(columns = {'label':'targets'}, inplace = True)




if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    if n_gpus < 1:
        print(f"Requires at least 2 GPUs to run, but got {n_gpus}.")
    else:
        #######################
        # generated by torchrun
        #######################
        local_rank = int(os.environ["LOCAL_RANK"])
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])

        if rank == 0:
            if n_gpus != int(os.environ['LOCAL_WORLD_SIZE']):
                print('not using all GPUs')
            print(f'num of workers: {NUM_WORKERS}')

        # initialize the process group
        # dist.init_process_group("nccl", rank=rank, world_size=world_size) # extra args taken care by torchrun 
        dist.init_process_group("nccl")

        run_id = datetime.now().strftime("%Y-%m-%d-%I:%M:%S_%p")

        # note we pass global ranks and not local ranks to the dataloaders
        train_dl, train_sampler, val_dl, val_sampler = datasets(train_df, TRAIN_BATCH, val_df, VAL_BATCH, tokenizer, MAX_LEN, rank, world_size)

        # Set up model
        model_class = ModelClass(model_name=MODEL, num_labels=NUM_LABELS, len_train_dl=len(train_dl), lr=LR, epochs=NUM_EPOCHS)

        model = model_class.model.to(local_rank)
        ddp_model = DDP(model, device_ids=[local_rank])
        optimizer, scheduler = model_class.opt_sch()

        torch.cuda.set_device(local_rank)
        print(f"host: {gethostname()}, rank: {rank}, local_rank: {local_rank}")

        if rank == 0:
            print(f"--> running with id {run_id}")
            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"--> {MODEL} has {total_params/1e6} Million params\n")

        trainer = TrainerClass(
            model=model,
            train_dataloader=train_dl,
            train_sampler=train_sampler,
            val_dataloader=val_dl,
            val_sampler=val_sampler,
            optimizer=optimizer,
            scheduler=scheduler,
            device=DEVICE,
            epochs=NUM_EPOCHS,
            rank=rank,
            local_rank=local_rank,
            run_id=run_id,
            enable_profiler=PROFILE
        )
        start_time = timer()

        train_results = trainer.fit()

        end_time = timer()

        dist.barrier()
        dist.destroy_process_group()
        # print out how long it took
        
        print(f"Total training time: {end_time-start_time:.3f} seconds")